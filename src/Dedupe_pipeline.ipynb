{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe7f2bb",
   "metadata": {},
   "source": [
    "## Imports and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2615a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a885bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10de697",
   "metadata": {},
   "source": [
    "## Create a corpus from all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877a63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads in data from all json files\n",
    "\n",
    "src = \"/workspaces/galeras-benchmark/datasets/code_smells\"\n",
    "\n",
    "arrOfData = []\n",
    "\n",
    "src_files = os.listdir(src)\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    src_files_2 = os.listdir(full_file_name)\n",
    "    for json_name in src_files_2:\n",
    "        json_file_name = os.path.join(full_file_name, json_name)\n",
    "        if os.path.isfile(json_file_name):\n",
    "            arrOfData += [json.load(open(json_file_name, encoding=\"utf-8\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0882a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for i in range(len(arrOfData)):\n",
    "    for j in range(len(arrOfData[i])):\n",
    "        words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(arrOfData[i][j][\"code\"])\n",
    "        new_words = [word for word, offset in words_with_offsets]\n",
    "        for word in new_words:\n",
    "            word_freqs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898651cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de9571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2adf53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e805581",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "\n",
    "#finding best pairs\n",
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "        \n",
    "merges = {}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1720ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges pairs\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "#finds most frequent pairs and adds them to splits\n",
    "def makePairs(splits, vSize):    \n",
    "    vocab_size = vSize\n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_freqs = compute_pair_freqs(splits)\n",
    "        best_pair = \"\"\n",
    "        max_freq = None\n",
    "        for pair, freq in pair_freqs.items():\n",
    "            if max_freq is None or max_freq < freq:\n",
    "                best_pair = pair\n",
    "                max_freq = freq\n",
    "        splits = merge_pair(*best_pair, splits)\n",
    "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "        vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9580b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replce 414 with length of vocab\n",
    "makePairs(splits, 414)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846333b7",
   "metadata": {},
   "source": [
    "## Deduplicate file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c843df8",
   "metadata": {},
   "source": [
    "#### manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae6babae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11a3dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call these functions with word freqs\n",
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def jaccardSet(list1, list2):\n",
    "    list1 = set(list1)\n",
    "    list2 = set(list2)\n",
    "    \n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2174654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isDupeHelper(data, i, j):\n",
    "    \n",
    "    #nvm this is broken, will only ever iterate a single time \n",
    "    if jaccard(data[i][\"BPE_tokens\"], data[j][\"BPE_tokens\"]) >= 0.7 and jaccardSet(data[i][\"BPE_tokens\"],data[j][\"BPE_tokens\"]) >= 0.8:\n",
    "        return True\n",
    "    else:\n",
    "        return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2949b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual dedupes\n",
    "#for code section\n",
    "src = \"/workspaces/galeras-benchmark/datasets/code_smells/aider/data_1.json\"\n",
    "data = json.load(open(src))\n",
    "\n",
    "tokenSet = set()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    #compare each code sections to every code section following it\n",
    "    tokenSet.add(data[i][\"code\"])\n",
    "    data[i].update({\"BPE_tokens\": tokenize(data[i][\"code\"])})\n",
    "\n",
    "#for i in len(list): iterate through every item\n",
    "\n",
    "def Deduper(data):\n",
    "    \n",
    "    retSet = set()\n",
    "    \n",
    "    for i in range(len(data)): \n",
    "        dupe = False\n",
    "        for j in range(i, len(data)):\n",
    "            if isDupeHelper(data, i, j):\n",
    "                retSet.add(data[i][\"code\"])\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "867cc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lset = Deduper(data)\n",
    "\n",
    "goodSet = tokenSet - Lset\n",
    "retFile = []\n",
    "\n",
    "for i in range(len(data)): \n",
    "    if data[i][\"code\"] in goodSet:\n",
    "        del data[i][\"BPE_tokens\"] #this is because python has an interpertation problem with the special characters used in BPE\n",
    "        retFile.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3efb49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_dedupe\", \"w\") as f:\n",
    "    json.dump(retFile, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815909c",
   "metadata": {},
   "source": [
    "#### with tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc26b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tool\n",
    "from dpu_utils.codeutils.deduplication import DuplicateDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25927075",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"/workspaces/galeras-benchmark/datasets/code_smells/aider/data_1.json\"\n",
    "#data = json.load(open(src), encoding=\"utf8\")\n",
    "data = json.load(open(src))\n",
    "\n",
    "\n",
    "dupes = DuplicateDetector()\n",
    "\n",
    "\n",
    "def Deduper(data):\n",
    "    dupes = DuplicateDetector()\n",
    "\n",
    "    totalSet = set()\n",
    "          \n",
    "    for i in range(len(data)):\n",
    "\n",
    "        if data[i][\"code\"] not in totalSet:   \n",
    "            dupes.add_file(data[i][\"code\"], tokenize(data[i][\"code\"]))\n",
    "\n",
    "        totalSet.add(data[i][\"code\"])\n",
    "        \n",
    "    exclude = dupes.compute_ids_to_exclude()\n",
    "    \n",
    "    retFileTool = []\n",
    "\n",
    "    for i in range(len(data)): \n",
    "        if data[i][\"code\"] not in exclude: \n",
    "            retFileTool.append(data[i])\n",
    "            \n",
    "    return retFileTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00a9a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "retFileTool = Deduper(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff422af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_dedupe\", \"w\") as f:\n",
    "    json.dump(retFile, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a4c0c",
   "metadata": {},
   "source": [
    "## For the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6b0eb",
   "metadata": {},
   "source": [
    "What deduplicating the entire dataset with this tool might look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16c7bece",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m totalSet:   \n\u001b[0;32m---> 17\u001b[0m         dupes\u001b[38;5;241m.\u001b[39madd_file(data[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m         totalSet\u001b[38;5;241m.\u001b[39madd(data[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#will throw value error if there are no dupes in file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splits):\n\u001b[1;32m      7\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m split[i] \u001b[38;5;241m==\u001b[39m pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m split[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m pair[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     10\u001b[0m             split \u001b[38;5;241m=\u001b[39m split[:i] \u001b[38;5;241m+\u001b[39m [merge] \u001b[38;5;241m+\u001b[39m split[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src = \"/workspaces/galeras-benchmark/datasets/code_smells\"\n",
    "\n",
    "src_files = os.listdir(src)\n",
    "\n",
    "\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(src, file_name)\n",
    "    src_files_2 = os.listdir(full_file_name)\n",
    "    for json_name in src_files_2:\n",
    "        json_file_name = os.path.join(full_file_name, json_name)\n",
    "        if os.path.isfile(json_file_name):\n",
    "            data = json.load(open(json_file_name, encoding=\"utf-8\"))\n",
    "            totalSet = set()\n",
    "            dupes = DuplicateDetector()\n",
    "            for i in range(len(data)):\n",
    "                if data[i][\"code\"] not in totalSet:   \n",
    "                    dupes.add_file(data[i][\"code\"], tokenize(data[i][\"code\"]))\n",
    "                    totalSet.add(data[i][\"code\"])\n",
    "               \n",
    "            #will throw value error if there are no dupes in file\n",
    "            try:\n",
    "                exclude = dupes.compute_ids_to_exclude()\n",
    "            except TypeError:\n",
    "                print('error')\n",
    "            \n",
    "            retFileTool = []\n",
    "\n",
    "            for i in range(len(data)): \n",
    "                if data[i][\"code\"] not in exclude: \n",
    "                    retFileTool.append(data[i])        \n",
    "\n",
    "            with open(json_file_name + \"Deduped.json\", \"w\") as f:\n",
    "                json.dump(retFile, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
